{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e03cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/gemma/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from rouge_score import rouge_scorer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae7d54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer, model, and data collator\n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8daa80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = load_from_disk(\"./tokenizedDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18e976c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized.train_test_split(test_size=0.99, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bad1990",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5095ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "   preds, labels = eval_preds\n",
    "\n",
    "   # decode preds and labels\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "   # rougeLSum expects newline after each sentence\n",
    "   decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "   decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "   result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "  \n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07da17f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "L_RATE = 3e-4\n",
    "BATCH_SIZE = 8\n",
    "PER_DEVICE_EVAL_BATCH = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "SAVE_TOTAL_LIM = 3\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"./results-small\",\n",
    "   learning_rate=L_RATE,\n",
    "   per_device_train_batch_size=BATCH_SIZE,\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "   weight_decay=WEIGHT_DECAY,\n",
    "   save_total_limit=SAVE_TOTAL_LIM,\n",
    "   num_train_epochs=NUM_EPOCHS,\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6d07e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_189306/2420821063.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_dataset[\"train\"],\n",
    "   eval_dataset=tokenized_dataset[\"test\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95c6b647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2646' max='2646' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2646/2646 02:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.335800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2646, training_loss=0.08512071867558602, metrics={'train_runtime': 145.2611, 'train_samples_per_second': 145.579, 'train_steps_per_second': 18.215, 'total_flos': 982756625743872.0, 'train_loss': 0.08512071867558602, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff13355d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checkpoint-2000', 'checkpoint-2500', 'checkpoint-2646', 'runs']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('results-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8db9e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint = \"./results/checkpoint-2646\"\n",
    "\n",
    "finetuned_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "tokenizer = T5Tokenizer.from_pretrained(last_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af30aadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> search(Blue car)</s>\n"
     ]
    }
   ],
   "source": [
    "inputs = \"Go look for the blue car\"\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "outputs = finetuned_model.generate(**inputs)\n",
    "answer = tokenizer.decode(outputs[0])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9eff753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> goto(Blue car)</s>\n"
     ]
    }
   ],
   "source": [
    "inputs = \"Go to the blue car\"\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "outputs = finetuned_model.generate(**inputs)\n",
    "answer = tokenizer.decode(outputs[0])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcc6ad93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> patrol(Blue car)</s>\n"
     ]
    }
   ],
   "source": [
    "inputs = \"Inspect the area around the blue car\"\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "outputs = finetuned_model.generate(**inputs)\n",
    "answer = tokenizer.decode(outputs[0])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bd67ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_subset(tokenized_dataset, subset_percentage=0.01):\n",
    "    \n",
    "    test_data = tokenized_dataset['test']\n",
    "    \n",
    "    #Select 1% of test data\n",
    "    subset_size = int(len(test_data) * subset_percentage)\n",
    "    test_subset = test_data.shuffle(seed=42).select(range(subset_size))\n",
    "    \n",
    "    print(f\"Original test data: {len(test_data):,} samples\")\n",
    "    print(f\"Test subset (1%): {len(test_subset):,} samples\")\n",
    "    \n",
    "    return test_subset\n",
    "\n",
    "def evaluate_model_on_subset(model, tokenizer, test_subset, num_samples=None):\n",
    "    if num_samples is None:\n",
    "        num_samples = len(test_subset)\n",
    "    else:\n",
    "        num_samples = min(num_samples, len(test_subset))\n",
    "    \n",
    "    print(f\"\\n=== EVALUATING ON {num_samples} SAMPLES ===\")\n",
    "    \n",
    "    #Metrics\n",
    "    exact_matches = 0\n",
    "    valid_commands = 0\n",
    "    rouge_scores = []\n",
    "    \n",
    "    #ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    #Track some examples\n",
    "    examples = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        sample = test_subset[i]\n",
    "        \n",
    "        #Decode input\n",
    "        input_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "        \n",
    "        #Get ground truth target\n",
    "        target_ids = [x for x in sample['labels'] if x != -100]\n",
    "        target_text = tokenizer.decode(target_ids, skip_special_tokens=True)\n",
    "        \n",
    "        #Make inference\n",
    "        inputs = tokenizer(\n",
    "            input_text, \n",
    "            return_tensors=\"pt\",\n",
    "            max_length=128,\n",
    "            truncation=True\n",
    "        ).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=48,\n",
    "                do_sample=False,  # Deterministic\n",
    "                num_beams=1,      # Fast generation\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        #Decode prediction\n",
    "        predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        #Calculate metrics\n",
    "        exact_match = predicted_text.strip() == target_text.strip()\n",
    "        if exact_match:\n",
    "            exact_matches += 1\n",
    "        \n",
    "        #Rouge-L score (longest common subsequence)\n",
    "        rouge_score = scorer.score(target_text, predicted_text)\n",
    "        rouge_scores.append(rouge_score['rougeL'].fmeasure)\n",
    "        \n",
    "        # Store examples for display\n",
    "        if i < 10:  # Show first 10 examples\n",
    "            examples.append({\n",
    "                'input': input_text,\n",
    "                'target': target_text,\n",
    "                'predicted': predicted_text,\n",
    "                'exact_match': exact_match,\n",
    "                #'valid_command': is_valid,\n",
    "                'rouge_l': rouge_score['rougeL'].fmeasure\n",
    "            })\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    exact_accuracy = exact_matches / num_samples\n",
    "    avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n=== EVALUATION RESULTS ===\")\n",
    "    print(f\"Exact Match Accuracy: {exact_accuracy:.3f} ({exact_matches}/{num_samples})\")\n",
    "    print(f\"Average ROUGE-L: {avg_rouge:.3f}\")\n",
    "    \n",
    "    print(f\"\\n=== SAMPLE PREDICTIONS ===\")\n",
    "    for i, ex in enumerate(examples):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  Input: {ex['input']}\")\n",
    "        print(f\"  Target: {ex['target']}\")\n",
    "        print(f\"  Predicted: {ex['predicted']}\")\n",
    "        print(f\"  Exact Match: {'✅' if ex['exact_match'] else '❌'}\")\n",
    "        print(f\"  ROUGE-L: {ex['rouge_l']:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'exact_accuracy': exact_accuracy,\n",
    "        'avg_rouge': avg_rouge,\n",
    "        'examples': examples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d93f54a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original test data: 697,924 samples\n",
      "Test subset (1%): 6,979 samples\n",
      "\n",
      "=== EVALUATING ON 1500 SAMPLES ===\n",
      "\n",
      "=== EVALUATION RESULTS ===\n",
      "Exact Match Accuracy: 1.000 (1500/1500)\n",
      "Average ROUGE-L: 1.000\n",
      "\n",
      "=== SAMPLE PREDICTIONS ===\n",
      "\n",
      "Example 1:\n",
      "  Input: Maintain vigilance in the vicinity of Human eye\n",
      "  Target: patrol(Human eye)\n",
      "  Predicted: patrol(Human eye)\n",
      "  Exact Match: ✅\n",
      "  ROUGE-L: 1.000\n",
      "\n",
      "Example 2:\n",
      "  Input: perform security rounds around Chainsaw\n",
      "  Target: patrol(Chainsaw)\n",
      "  Predicted: patrol(Chainsaw)\n",
      "  Exact Match: ✅\n",
      "  ROUGE-L: 1.000\n",
      "\n",
      "Example 3:\n",
      "  Input: dig up Whisk\n",
      "  Target: search(Whisk)\n",
      "  Predicted: search(Whisk)\n",
      "  Exact Match: ✅\n",
      "  ROUGE-L: 1.000\n",
      "\n",
      "Example 4:\n",
      "  Input: Provide comprehensive coverage around Alpaca\n",
      "  Target: patrol(Alpaca)\n",
      "  Predicted: patrol(Alpaca)\n",
      "  Exact Match: ✅\n",
      "  ROUGE-L: 1.000\n",
      "\n",
      "Example 5:\n",
      "  Input: yo, find Office supplies\n",
      "  Target: search(Office supplies)\n",
      "  Predicted: search(Office supplies)\n",
      "  Exact Match: ✅\n",
      "  ROUGE-L: 1.000\n",
      "\n",
      "Example 6:\n",
      "  Input: Check all quadrants near Measuring cup\n",
      "  Target: patrol(Measuring cup)\n",
      "  Predicted: patrol(Measuring cup)\n",
      "  Exact Match: ✅\n",
      "  ROUGE-L: 1.000\n",
      "\n",
      "Example 7:\n",
      "  Input: Investigate and retrieve the Squid\n",
      "  Target: search(Squid)\n",
      "  Predicted: search(Squid)\n",
      "  Exact Match: ✅\n",
      "  ROUGE-L: 1.000\n",
      "\n",
      "Example 8:\n",
      "  Input: Advance to Computer keyboard\n",
      "  Target: goto(Computer keyboard)\n",
      "  Predicted: goto(Computer keyboard)\n",
      "  Exact Match: ✅\n",
      "  ROUGE-L: 1.000\n",
      "\n",
      "Example 9:\n",
      "  Input: go and find Swim cap\n",
      "  Target: search(Swim cap)\n",
      "  Predicted: search(Swim cap)\n",
      "  Exact Match: ✅\n",
      "  ROUGE-L: 1.000\n",
      "\n",
      "Example 10:\n",
      "  Input: Head towards Cake\n",
      "  Target: goto(Cake)\n",
      "  Predicted: goto(Cake)\n",
      "  Exact Match: ✅\n",
      "  ROUGE-L: 1.000\n"
     ]
    }
   ],
   "source": [
    "tokenized_split = tokenized.train_test_split(test_size=0.99, seed=42)\n",
    "\n",
    "test_subset = prepare_test_subset(tokenized_split, subset_percentage=0.01)\n",
    "test_data = tokenized_split['test']\n",
    "test_subset = test_data.shuffle(seed=42).select(range(min(1500, len(test_data))))\n",
    "results = evaluate_model_on_subset(finetuned_model, tokenizer, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e5438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "gemma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
